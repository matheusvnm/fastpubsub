# Introduction to Google Pub/Sub

Google Cloud Pub/Sub is a fully-managed, real-time messaging service designed for asynchronous communication between independent applications. Its primary function is to **decouple** services that produce events, known as **Publishers**, from the services that process them, called **Subscribers**. This separation creates a robust and scalable architecture where producers can send data without any knowledge of who will receive it or when it will be processed, enabling high-throughput and reliable event-driven systems.

The entire system operates around a few core components that manage the flow of data. A Publisher application creates a **message**, which contains a data payload and optional metadata. This message is then sent to a specific **Topic**, a named channel for a particular category of events, such as `new-orders` or `user-signups`. To receive these messages, a **Subscriber** application connects to a **Subscription**, which is a named resource representing a stream of messages from a single topic. A topic can have multiple subscriptions, allowing the same stream of events to be consumed independently by different applications.

A defining feature of Pub/Sub is its guarantee of **at-least-once message delivery**, managed through a sophisticated acknowledgment system. When a subscriber receives a message, Pub/Sub considers it "in flight." The subscriber has a configurable period, the **acknowledgment deadline**, to process the data and send an **acknowledgment (ack)** back. If the ack is received, the message is permanently removed from the subscription's backlog. If the deadline expires without an ack (e.g., the subscriber crashes), Pub/Sub assumes delivery failed and will redeliver the message. Subscribers can also proactively signal failure by sending a **negative acknowledgment (nack)**, prompting an immediate redelivery.

To handle messages that consistently fail to be processed, a subscription can be configured with a **Dead-Letter Topic (DLT)**. After a set number of delivery attempts, Pub/Sub forwards the "poison pill" message to this separate topic for later inspection. This prevents a single bad message from halting the entire processing pipeline.

---

## Pub/Sub Communication Patterns

The architecture of topics and subscriptions in Pub/Sub enables highly flexible communication patterns. When designing your system, you'll likely use one of the following strategies:

**One-to-Many (Fan-out):** A single message published to a topic is delivered to multiple subscriber systems, each with its own subscription. This allows different services to react to the same event in parallel. For example, a single `order-placed` event can be sent to inventory, notification, and analytics services simultaneously.

**Many-to-One (Fan-in):** Multiple publisher systems send messages to a single topic, which are then processed by a single logical subscriber service. This pattern is used to centralize and aggregate data from various sources, such as collecting logs or metrics from a fleet of microservices into one processing pipeline.

**Many-to-Many:** This combines the two previous patterns. Multiple publishers send messages to a topic, and multiple subscriber systems consume these events for distinct purposes. It's the most flexible model, creating a fully decoupled event bus for complex, large-scale architectures.

For a more detailed exploration of these concepts and other fundamentals, refer to the official [Google Cloud documentation](https://cloud.google.com/pubsub/docs/pubsub-basics).

---

## Subscription Delivery Types

A key part of a subscriber's design is choosing *how* it receives messages. Pub/Sub offers three distinct delivery methods to suit different application needs.

### Push Delivery
With **push** delivery, Pub/Sub initiates contact and sends messages to your subscriber application at a pre-configured webhook endpoint. You provide a publicly accessible HTTPS URL, and Pub/Sub will POST messages to it as they arrive.

* **How it works:** Pub/Sub acts as an HTTP client, sending a POST request with the message data to your endpoint. Your application must acknowledge or negactively acknowledge the message by returning a HTTP status codes (e.g., `200 OK`, `500 Internal Server Error`).
* **When to use it:** This method is perfect for serverless applications or REST APIs (like your FastAPI app) that are already designed to receive webhooks. It simplifies the subscriber logic since you don't need to implement a polling mechanism.
* **Pitfalls:** This model's primary challenge lies in its requirement for a accessible HTTPS endpoint. This introduces significant security (if its public) or networking complexity (if its private), as the application should implement some authentication logic to validate incoming request tokens. Furthermore, the subscriber has no control over the delivery rate, making it vulnerable to the "thundering herd" problem, where a sudden burst of messages can overwhelm the endpoint's capacity.

### Pull Delivery

The pull delivery strategy can be implemented using two different variations a (synchronous and asynchronous) as follows:

**Synchronous Pull** is a classic request-response model where your application explicitly asks Pub/Sub for messages. This is based on a unary RPC call.
* **How it works:** The subscriber client calls a method (e.g., `subscriber.pull()` in Python) to request a batch of messages. The application code is responsible for creating a loop to continuously poll for new messages and for acknowledging each one after processing.
* **When to use it:** This approach is best for short-lived jobs, simple scripts, or scenarios where you need direct, fine-grained control over when messages are fetched and the rate of consumption.
* **Pitfalls:** Since this approach has the most control over the polling loop, it also means that the user application must control all the lifecycle of the message, which includes exception handling, message consumption flow and ack deadline extensions.


**Streaming Pull** is the most performant for most applications. It establishes a long-lived, bidirectional gRPC stream between your client and the Pub/Sub service, allowing messages to be pushed from the service to your client with very low latency.

* **How it works:** High-level client libraries (e.g., using `subscriber.subscribe()` in Python) abstract this complexity away. You simply register a callback function to process messages. The library manages the persistent stream in the background, continuously receiving messages, and invoking your callback for each one. It also automatically handles flow control, extending ack deadlines, and sending acknowledgements in parallel.
* **When to use it:** This is the ideal choice for long-running services that require high throughput and low latency. It provides the most efficient use of resources and is the standard for building robust, scalable subscriber applications.
* **Pitfalls:** In the context of Python applications the Streaming Pull strategy does not allow you to use long-lived `asyncio` event loops to apply full concurency. By default it uses several threads to process the messages. Hence, due to the thread-unsafe nature of `asyncio`, the user either must create the loops everytime arives or create long-live loop in the thread context usually via [**ContextVars**](https://docs.python.org/3/library/contextvars.html). This limitation is discussed deeply in following sections.

---

## Pub/Sub vs. Kafka: The Parallelism Model

The way Pub/Sub handles parallelism is a significant differentiator when compared to technologies like Apache Kafka.

Kafka's parallelism is fundamentally based on **partitions**. A Kafka topic is divided into a fixed number of partitions. Within a consumer group, each partition can only be assigned to a single consumer instance at any given time. This design inherently limits the maximum parallelism of a consumer group to the number of partitions in the topic. To scale beyond this limit, you must manually increase the topic's partition count, which can be an operational burden.


In contrast, Pub/Sub employs a more dynamic and elastic model of **per-message parallelism**. It abstracts the concept of partitions away from the user, instead load-balancing individual messages across all available subscriber clients connected to a single subscription. This allows for massive, fine-grained scaling without manual intervention. You can have thousands of subscriber clients pulling from the same subscription, and Pub/Sub will automatically and efficiently distribute the workload among them. This makes scaling your consumer fleet up or down a much simpler and more fluid process, perfectly aligning with the dynamic nature of cloud-native applications.



Of course. Here is a recap section that summarizes the key takeaways from your documentation.

## Recap

In this document, we've explored the fundamentals of Google Cloud Pub/Sub. Here are the key points to remember:

- **Core Purpose**: Pub/Sub is a messaging service that decouples event producers (Publishers) from consumers (Subscribers) using Topics and Subscriptions, enabling scalable and reliable asynchronous communication.

- **Reliability**: It guarantees at-least-once delivery through an acknowledgment system (ack/nack). Failed messages can be automatically moved to a Dead-Letter Topic (DLT) to prevent processing blocks.

- **Communication Patterns**: Pub/Sub supports flexible architectures like fan-out (one-to-many), fan-in (many-to-one), and many-to-many, allowing you to build complex event-driven systems.

- **Delivery Types**: Subscribers can receive messages in three ways:

    - **Push**: Pub/Sub sends messages to a webhook (ideal for serverless).
    - **Synchronous Pull**: The client explicitly requests messages (good for simple jobs).
    - **Streaming Pull**: A persistent connection streams messages with low latency (recommended for most high-throughput applications).

- **Scalability Advantage**: Unlike Kafka's rigid partition-based parallelism, Pub/Sub offers dynamic per-message parallelism, allowing you to scale your subscribers fluidly without needing to manage partitions.
